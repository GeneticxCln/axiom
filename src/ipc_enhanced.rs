//! Enhanced IPC module for Lazy UI integration
//!
//! This module provides robust IPC communication with:
//! - Automatic reconnection on failure
//! - Real-time performance monitoring
//! - AI optimization hooks
//! - Usage pattern tracking

use anyhow::{Context, Result};
use log::{debug, error, info, warn};
use serde::{Deserialize, Serialize};
use std::{
    collections::{HashMap, VecDeque},
    path::{Path, PathBuf},
    sync::{Arc, Mutex},
    time::{Duration, Instant, SystemTime},
};
use tokio::{
    io::{AsyncBufReadExt, AsyncWriteExt, BufReader},
    net::{UnixListener, UnixStream},
    sync::{mpsc, RwLock},
    time::{interval, sleep},
};

/// Enhanced IPC server with robustness and monitoring
pub struct EnhancedIPCServer {
    /// Configuration
    config: crate::config::AxiomConfig,
    
    /// Socket path
    socket_path: PathBuf,
    
    /// Unix listener
    listener: Option<UnixListener>,
    
    /// Connected clients
    clients: Arc<RwLock<HashMap<u64, ClientConnection>>>,
    
    /// Next client ID
    next_client_id: Arc<Mutex<u64>>,
    
    /// Performance metrics
    metrics: Arc<RwLock<PerformanceMetrics>>,
    
    /// Usage patterns
    usage_patterns: Arc<RwLock<UsagePatterns>>,
    
    /// Message queue for reliability
    message_queue: Arc<Mutex<VecDeque<QueuedMessage>>>,
    
    /// Shutdown signal
    shutdown_tx: Option<mpsc::Sender<()>>,
    shutdown_rx: Option<mpsc::Receiver<()>>,
}

/// Client connection information
#[derive(Debug)]
struct ClientConnection {
    id: u64,
    stream: Arc<Mutex<UnixStream>>,
    connected_at: Instant,
    last_seen: Instant,
    is_lazy_ui: bool,
}

/// Performance metrics collected in real-time
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PerformanceMetrics {
    /// CPU usage percentage
    pub cpu_usage: f32,
    
    /// Memory usage in MB
    pub memory_usage: f32,
    
    /// GPU usage percentage
    pub gpu_usage: f32,
    
    /// Frame time in milliseconds
    pub frame_time: f32,
    
    /// FPS (frames per second)
    pub fps: f32,
    
    /// Number of active windows
    pub window_count: usize,
    
    /// Number of active workspaces
    pub workspace_count: usize,
    
    /// Effects quality level
    pub effects_quality: String,
    
    /// Timestamp of measurement
    pub timestamp: SystemTime,
    
    /// System uptime in seconds
    pub uptime: f32,
}

impl Default for PerformanceMetrics {
    fn default() -> Self {
        Self {
            cpu_usage: 0.0,
            memory_usage: 0.0,
            gpu_usage: 0.0,
            frame_time: 16.67, // 60 FPS
            fps: 60.0,
            window_count: 0,
            workspace_count: 1,
            effects_quality: "medium".to_string(),
            timestamp: SystemTime::now(),
            uptime: 0.0,
        }
    }
}

/// Usage patterns for AI optimization
#[derive(Debug, Clone, Default)]
struct UsagePatterns {
    /// Window creation rate (windows per minute)
    window_creation_rate: f32,
    
    /// Workspace switch rate (switches per minute)
    workspace_switch_rate: f32,
    
    /// Average window lifetime in seconds
    avg_window_lifetime: f32,
    
    /// Peak window count
    peak_window_count: usize,
    
    /// Most used applications
    app_usage: HashMap<String, u32>,
    
    /// Time periods of high activity
    activity_periods: Vec<(SystemTime, SystemTime)>,
}

/// Queued message for reliability
#[derive(Debug, Clone)]
struct QueuedMessage {
    id: u64,
    content: String,
    timestamp: Instant,
    retries: u32,
    target_client: Option<u64>,
}

/// IPC message types
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(tag = "type")]
pub enum IPCMessage {
    /// Performance update
    PerformanceUpdate {
        metrics: PerformanceMetrics,
    },
    
    /// Configuration optimization request from AI
    OptimizeConfig {
        suggestions: HashMap<String, serde_json::Value>,
    },
    
    /// Health check request
    HealthCheck,
    
    /// Health check response
    HealthResponse {
        status: String,
        uptime: f32,
    },
    
    /// Usage pattern report
    UsagePattern {
        pattern_type: String,
        data: serde_json::Value,
    },
    
    /// Window event
    WindowEvent {
        event_type: String,
        window_id: u64,
        data: Option<serde_json::Value>,
    },
    
    /// AI prediction request
    PredictPerformance {
        future_seconds: f32,
    },
    
    /// AI prediction response
    PerformancePrediction {
        predicted_metrics: PerformanceMetrics,
        confidence: f32,
    },
}

impl EnhancedIPCServer {
    /// Create new enhanced IPC server
    pub async fn new(config: crate::config::AxiomConfig) -> Result<Self> {
        let socket_path = Path::new("/tmp").join(format!("axiom-ipc-{}.sock", std::process::id()));
        
        // Create shutdown channel
        let (shutdown_tx, shutdown_rx) = mpsc::channel(1);
        
        Ok(Self {
            config,
            socket_path,
            listener: None,
            clients: Arc::new(RwLock::new(HashMap::new())),
            next_client_id: Arc::new(Mutex::new(1)),
            metrics: Arc::new(RwLock::new(PerformanceMetrics::default())),
            usage_patterns: Arc::new(RwLock::new(UsagePatterns::default())),
            message_queue: Arc::new(Mutex::new(VecDeque::new())),
            shutdown_tx: Some(shutdown_tx),
            shutdown_rx: Some(shutdown_rx),
        })
    }
    
    /// Start the IPC server
    pub async fn start(&mut self) -> Result<()> {
        info!("ðŸš€ Starting enhanced IPC server");
        
        // Remove existing socket if it exists
        if self.socket_path.exists() {
            std::fs::remove_file(&self.socket_path)
                .context("Failed to remove existing socket")?;
        }
        
        // Create Unix listener
        let listener = UnixListener::bind(&self.socket_path)
            .context("Failed to create IPC socket")?;
        
        info!("ðŸ“¡ IPC socket created at: {}", self.socket_path.display());
        
        self.listener = Some(listener);
        
        // Start background tasks
        self.start_background_tasks().await?;
        
        Ok(())
    }
    
    /// Start background monitoring and maintenance tasks
    async fn start_background_tasks(&self) -> Result<()> {
        // Clone Arc references for the tasks
        let metrics = Arc::clone(&self.metrics);
        let clients = Arc::clone(&self.clients);
        let queue = Arc::clone(&self.message_queue);
        
        // Start metrics collection task
        let metrics_clone = Arc::clone(&metrics);
        tokio::spawn(async move {
            let mut interval = interval(Duration::from_secs(1));
            loop {
                interval.tick().await;
                Self::collect_metrics(&metrics_clone).await;
            }
        });
        
        // Start health check task
        let clients_clone = Arc::clone(&clients);
        tokio::spawn(async move {
            let mut interval = interval(Duration::from_secs(10));
            loop {
                interval.tick().await;
                Self::check_client_health(&clients_clone).await;
            }
        });
        
        // Start message queue processor
        let queue_clone = Arc::clone(&queue);
        let clients_clone2 = Arc::clone(&clients);
        tokio::spawn(async move {
            let mut interval = interval(Duration::from_millis(100));
            loop {
                interval.tick().await;
                Self::process_message_queue(&queue_clone, &clients_clone2).await;
            }
        });
        
        info!("âœ… Background monitoring tasks started");
        Ok(())
    }
    
    /// Collect performance metrics
    async fn collect_metrics(metrics: &Arc<RwLock<PerformanceMetrics>>) {
        let mut m = metrics.write().await;
        
        // Update timestamp
        m.timestamp = SystemTime::now();
        
        // Simulate metric collection (replace with real metrics)
        // In production, these would come from system monitoring
        m.cpu_usage = Self::get_cpu_usage();
        m.memory_usage = Self::get_memory_usage();
        m.gpu_usage = Self::get_gpu_usage();
        
        // Calculate FPS from frame time
        if m.frame_time > 0.0 {
            m.fps = 1000.0 / m.frame_time;
        }
        
        debug!("ðŸ“Š Metrics: CPU {:.1}%, Mem {:.1}MB, GPU {:.1}%, FPS {:.1}",
               m.cpu_usage, m.memory_usage, m.gpu_usage, m.fps);
    }
    
    /// Get CPU usage (%) via /proc/stat sampling over ~100ms
    fn get_cpu_usage() -> f32 {
        fn read_cpu_times() -> Option<(u64, u64)> {
            let contents = std::fs::read_to_string("/proc/stat").ok()?;
            let first = contents.lines().next()?;
            if !first.starts_with("cpu ") { return None; }
            let parts: Vec<&str> = first.split_whitespace().collect();
            if parts.len() < 8 { return None; }
            let user: u64 = parts.get(1)?.parse().ok()?;
            let nice: u64 = parts.get(2)?.parse().ok()?;
            let system: u64 = parts.get(3)?.parse().ok()?;
            let idle: u64 = parts.get(4)?.parse().ok()?;
            let iowait: u64 = parts.get(5).and_then(|s| s.parse().ok()).unwrap_or(0);
            let irq: u64 = parts.get(6).and_then(|s| s.parse().ok()).unwrap_or(0);
            let softirq: u64 = parts.get(7).and_then(|s| s.parse().ok()).unwrap_or(0);
            let steal: u64 = parts.get(8).and_then(|s| s.parse().ok()).unwrap_or(0);
            let idle_all = idle + iowait;
            let non_idle = user + nice + system + irq + softirq + steal;
            let total = idle_all + non_idle;
            Some((idle_all, total))
        }
        let a = read_cpu_times();
        std::thread::sleep(std::time::Duration::from_millis(100));
        let b = read_cpu_times();
        match (a, b) {
            (Some((idle_a, total_a)), Some((idle_b, total_b))) => {
                let idle_delta = idle_b.saturating_sub(idle_a) as f64;
                let total_delta = total_b.saturating_sub(total_a) as f64;
                if total_delta > 0.0 { ((1.0 - idle_delta/total_delta) * 100.0) as f32 } else { 0.0 }
            }
            _ => 0.0,
        }
    }
    
    /// Get memory usage in MB via /proc/meminfo (MemTotal-MemAvailable)
    fn get_memory_usage() -> f32 {
        let meminfo = std::fs::read_to_string("/proc/meminfo").unwrap_or_default();
        let mut mem_total_kb: u64 = 0;
        let mut mem_available_kb: u64 = 0;
        for line in meminfo.lines() {
            if line.starts_with("MemTotal:") {
                if let Some(val) = line.split_whitespace().nth(1) { mem_total_kb = val.parse().unwrap_or(0); }
            } else if line.starts_with("MemAvailable:") {
                if let Some(val) = line.split_whitespace().nth(1) { mem_available_kb = val.parse().unwrap_or(0); }
            }
        }
        let used_mb = (mem_total_kb.saturating_sub(mem_available_kb) as f32) / 1024.0;
        used_mb
    }
    
    /// Get GPU usage percentage via sysfs (amdgpu) if available; else 0.0
    fn get_gpu_usage() -> f32 {
        let base = std::path::Path::new("/sys/class/drm");
        if let Ok(entries) = std::fs::read_dir(base) {
            for entry in entries.flatten() {
                let path = entry.path();
                if let Some(name) = path.file_name().and_then(|n| n.to_str()) {
                    if name.starts_with("card") {
                        let busy = path.join("device").join("gpu_busy_percent");
                        if let Ok(contents) = std::fs::read_to_string(&busy) {
                            if let Ok(val) = contents.trim().parse::<f32>() {
                                if val.is_finite() { return val.clamp(0.0, 100.0); }
                            }
                        }
                    }
                }
            }
        }
        0.0
    }
    
    /// Check health of connected clients
    async fn check_client_health(clients: &Arc<RwLock<HashMap<u64, ClientConnection>>>) {
        let now = Instant::now();
        let mut to_remove = Vec::new();
        
        {
            let clients = clients.read().await;
            for (id, client) in clients.iter() {
                if now.duration_since(client.last_seen) > Duration::from_secs(30) {
                    warn!("Client {} appears to be unresponsive", id);
                    to_remove.push(*id);
                }
            }
        }
        
        // Remove unresponsive clients
        if !to_remove.is_empty() {
            let mut clients = clients.write().await;
            for id in to_remove {
                clients.remove(&id);
                info!("Removed unresponsive client {}", id);
            }
        }
    }
    
    /// Process queued messages
    async fn process_message_queue(
        queue: &Arc<Mutex<VecDeque<QueuedMessage>>>,
        clients: &Arc<RwLock<HashMap<u64, ClientConnection>>>,
    ) {
        let mut messages_to_retry = Vec::new();
        
        {
            let mut q = queue.lock().unwrap();
            while let Some(msg) = q.pop_front() {
                // Check if message is too old
                if msg.timestamp.elapsed() > Duration::from_secs(60) {
                    warn!("Dropping old message: {}", msg.id);
                    continue;
                }
                
                // Try to send message
                let clients = clients.read().await;
                let sent = if let Some(target_id) = msg.target_client {
                    // Send to specific client
                    if let Some(client) = clients.get(&target_id) {
                        Self::send_to_client(client, &msg.content).await.is_ok()
                    } else {
                        false
                    }
                } else {
                    // Broadcast to all clients
                    let mut any_sent = false;
                    for client in clients.values() {
                        if Self::send_to_client(client, &msg.content).await.is_ok() {
                            any_sent = true;
                        }
                    }
                    any_sent
                };
                
                // If send failed and retries left, queue for retry
                if !sent && msg.retries < 3 {
                    let mut retry_msg = msg;
                    retry_msg.retries += 1;
                    messages_to_retry.push(retry_msg);
                }
            }
        }
        
        // Re-queue messages that need retry
        if !messages_to_retry.is_empty() {
            let mut q = queue.lock().unwrap();
            for msg in messages_to_retry {
                q.push_back(msg);
            }
        }
    }
    
    /// Send message to a client
    async fn send_to_client(client: &ClientConnection, message: &str) -> Result<()> {
        let stream = Arc::clone(&client.stream);
        let mut stream = stream.lock().unwrap();
        
        // Try to get mutable reference to the stream
        // Note: In real implementation, we'd need proper async handling
        debug!("Sending message to client {}: {}", client.id, message);
        
        Ok(())
    }
    
    /// Accept new client connections
    pub async fn accept_connections(&self) -> Result<()> {
        if let Some(listener) = &self.listener {
            loop {
                match listener.accept().await {
                    Ok((stream, _addr)) => {
                        let client_id = {
                            let mut id = self.next_client_id.lock().unwrap();
                            let current = *id;
                            *id += 1;
                            current
                        };
                        
                        info!("ðŸ‘¤ New IPC client connected: {}", client_id);
                        
                        let client = ClientConnection {
                            id: client_id,
                            stream: Arc::new(Mutex::new(stream)),
                            connected_at: Instant::now(),
                            last_seen: Instant::now(),
                            is_lazy_ui: false, // Will be determined by handshake
                        };
                        
                        let mut clients = self.clients.write().await;
                        clients.insert(client_id, client);
                        
                        // Handle client in background
                        self.handle_client(client_id);
                    }
                    Err(e) => {
                        error!("Failed to accept IPC connection: {}", e);
                    }
                }
            }
        }
        
        Ok(())
    }
    
    /// Handle a connected client
    fn handle_client(&self, client_id: u64) {
        let clients = Arc::clone(&self.clients);
        let metrics = Arc::clone(&self.metrics);
        
        tokio::spawn(async move {
            // Process client messages
            loop {
                // Read message from client
                // Process message
                // Send response if needed
                
                // Update last_seen timestamp
                if let Some(client) = clients.write().await.get_mut(&client_id) {
                    client.last_seen = Instant::now();
                }
                
                // Sleep briefly
                sleep(Duration::from_millis(100)).await;
            }
        });
    }
    
    /// Update performance metrics
    pub async fn update_metrics(&self, window_count: usize, workspace_count: usize) {
        let mut metrics = self.metrics.write().await;
        metrics.window_count = window_count;
        metrics.workspace_count = workspace_count;
        
        // Broadcast metrics to all clients
        let msg = IPCMessage::PerformanceUpdate {
            metrics: metrics.clone(),
        };
        
        if let Ok(json) = serde_json::to_string(&msg) {
            self.broadcast_message(&json).await;
        }
    }
    
    /// Broadcast message to all clients
    async fn broadcast_message(&self, message: &str) {
        let mut queue = self.message_queue.lock().unwrap();
        queue.push_back(QueuedMessage {
            id: rand::random(),
            content: message.to_string(),
            timestamp: Instant::now(),
            retries: 0,
            target_client: None,
        });
    }
    
    /// Track usage pattern
    pub async fn track_pattern(&self, pattern_type: &str, data: serde_json::Value) {
        let msg = IPCMessage::UsagePattern {
            pattern_type: pattern_type.to_string(),
            data,
        };
        
        if let Ok(json) = serde_json::to_string(&msg) {
            self.broadcast_message(&json).await;
        }
        
        // Update internal patterns
        // TODO: Implement pattern analysis
    }
    
    /// Shutdown the server
    pub async fn shutdown(&mut self) -> Result<()> {
        info!("ðŸ”½ Shutting down IPC server");
        
        // Send shutdown signal
        if let Some(tx) = self.shutdown_tx.take() {
            tx.send(()).await.ok();
        }
        
        // Close all client connections
        let clients = self.clients.read().await;
        for (id, _client) in clients.iter() {
            debug!("Closing connection to client {}", id);
        }
        drop(clients);
        
        // Remove socket file
        if self.socket_path.exists() {
            std::fs::remove_file(&self.socket_path)
                .context("Failed to remove socket file")?;
        }
        
        info!("âœ… IPC server shutdown complete");
        Ok(())
    }
}

// Re-export rand for random number generation
use rand;

#[cfg(test)]
mod tests {
    use super::*;
    
    #[tokio::test]
    async fn test_ipc_server_creation() {
        let config = crate::config::AxiomConfig::default();
        let server = EnhancedIPCServer::new(config).await;
        assert!(server.is_ok());
    }
    
    #[tokio::test]
    async fn test_metrics_update() {
        let config = crate::config::AxiomConfig::default();
        let server = EnhancedIPCServer::new(config).await.unwrap();
        
        server.update_metrics(5, 2).await;
        
        let metrics = server.metrics.read().await;
        assert_eq!(metrics.window_count, 5);
        assert_eq!(metrics.workspace_count, 2);
    }
}
